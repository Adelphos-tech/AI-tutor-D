# üöÄ Real-Time Voice AI Backend

Ultra-low latency voice pipeline with **<1 second end-to-end latency**

## Architecture

```
Client (Browser)
    ‚Üì WebSocket
FastAPI Backend
    ‚îú‚îÄ Deepgram Nova-3 (STT) ‚îÄ‚îÄ‚Üí Real-time transcription
    ‚îú‚îÄ Gemini 2.5 Pro (LLM) ‚îÄ‚îÄ‚îÄ‚Üí Streaming text generation
    ‚îî‚îÄ Deepgram Aura-1 (TTS) ‚îÄ‚îÄ‚Üí Real-time audio synthesis
    ‚Üì WebSocket
Client (Browser) ‚îÄ‚îÄ‚Üí Instant audio playback
```

## Features

‚úÖ **Zero Buffering** - Data forwarded immediately between stages
‚úÖ **Streaming Pipeline** - STT ‚Üí LLM ‚Üí TTS all stream concurrently
‚úÖ **Barge-in Support** - Interrupt AI mid-sentence
‚úÖ **Auto-reconnect** - Handles dropped connections
‚úÖ **Async Architecture** - Concurrent stream management with asyncio
‚úÖ **Production Ready** - Error handling, logging, health checks

## Performance

| Metric | Target | Typical |
|--------|--------|---------|
| **STT Latency** | <200ms | 150ms |
| **LLM First Token** | <500ms | 400ms |
| **TTS Latency** | <300ms | 250ms |
| **End-to-End** | <1s | 800ms |

## Setup

### 1. Install Dependencies

```bash
cd voice-backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. Configure Environment

Edit `.env`:
```bash
DEEPGRAM_API_KEY=your_deepgram_key
GEMINI_API_KEY=your_gemini_key
```

### 3. Start Server

```bash
./start.sh
# Or manually:
python main.py
```

Server runs on `http://localhost:8000`

## API Endpoints

### WebSocket: `/ws`

**Connect:**
```javascript
const ws = new WebSocket('ws://localhost:8000/ws')
```

**Send Audio:**
```javascript
ws.send(JSON.stringify({
  type: 'audio',
  data: base64_audio_chunk  // 16kHz PCM, Int16
}))
```

**Receive Messages:**
```javascript
ws.onmessage = (event) => {
  const message = JSON.parse(event.data)
  
  switch (message.type) {
    case 'status':
      // 'connected', 'generating', 'complete', 'interrupted'
      console.log('Status:', message.data)
      break
      
    case 'transcript':
      // Real-time transcription
      console.log('Transcript:', message.data.text)
      console.log('Is final:', message.data.is_final)
      break
      
    case 'text_chunk':
      // Streaming LLM response
      console.log('AI text:', message.data)
      break
      
    case 'audio':
      // Audio chunk (24kHz PCM, Int16, base64)
      playAudio(message.data)
      break
      
    case 'error':
      console.error('Error:', message.data)
      break
  }
}
```

**Interrupt:**
```javascript
ws.send(JSON.stringify({
  type: 'interrupt'
}))
```

### REST: `/health`

Health check endpoint:
```bash
curl http://localhost:8000/health
```

## Audio Specifications

### Input (Client ‚Üí Backend)
- **Format**: PCM Int16
- **Sample Rate**: 16kHz
- **Channels**: 1 (mono)
- **Encoding**: Base64

### Output (Backend ‚Üí Client)
- **Format**: PCM Int16
- **Sample Rate**: 24kHz
- **Channels**: 1 (mono)
- **Encoding**: Base64

## Pipeline Flow

### 1. Audio Input
```
User speaks ‚Üí Mic captures ‚Üí 16kHz PCM ‚Üí Base64 ‚Üí WebSocket
```

### 2. Speech-to-Text (Deepgram Nova-3)
```
Audio chunks ‚Üí Deepgram STT ‚Üí Interim transcripts ‚Üí Final transcript
```

### 3. LLM Generation (Gemini 2.5 Pro)
```
Final transcript ‚Üí Gemini streaming ‚Üí Text chunks (immediate)
```

### 4. Text-to-Speech (Deepgram Aura-1)
```
Text chunks ‚Üí Deepgram TTS ‚Üí Audio chunks ‚Üí WebSocket ‚Üí Client
```

### 5. Playback
```
Audio chunks ‚Üí Decode base64 ‚Üí PCM buffer ‚Üí AudioContext ‚Üí Speakers
```

## Interrupt/Barge-in

When user speaks while AI is talking:

1. Client detects new audio input
2. Sends `{type: 'interrupt'}` message
3. Backend:
   - Cancels ongoing Gemini generation
   - Stops TTS streaming
   - Clears audio buffers
   - Sends `{type: 'status', data: 'interrupted'}`
4. Client clears playback queue
5. New user input processed immediately

## Error Handling

### Connection Errors
```python
try:
    await pipeline.initialize()
except Exception as e:
    logger.error(f"Failed to initialize: {e}")
    await websocket.send_json({
        "type": "error",
        "data": str(e)
    })
```

### Stream Reconnection
- Deepgram STT: Auto-reconnects on disconnect
- Gemini: Retries with exponential backoff
- WebSocket: Client handles reconnection

### Graceful Shutdown
```python
finally:
    await pipeline.cleanup()
    # Closes all streaming connections
```

## Logging

Structured logging with timestamps:
```
2025-11-14 11:56:00 - INFO - üîå Client connected
2025-11-14 11:56:00 - INFO - ‚úÖ Deepgram STT (Nova-3) connected
2025-11-14 11:56:00 - INFO - ‚úÖ Gemini 2.5 Pro initialized
2025-11-14 11:56:01 - INFO - üìù Final transcript: Hello
2025-11-14 11:56:01 - INFO - üß† Generating response for: Hello
2025-11-14 11:56:02 - INFO - üîä Converting to speech: Hello! I'm Alex...
2025-11-14 11:56:02 - INFO - ‚úÖ TTS chunk complete
```

## Development

### Run with Auto-reload
```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Test WebSocket
```bash
# Install wscat
npm install -g wscat

# Connect
wscat -c ws://localhost:8000/ws

# Send test message
{"type": "audio", "data": "base64_audio_here"}
```

### Monitor Logs
```bash
tail -f logs/voice-backend.log
```

## Deployment

### Docker
```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Environment Variables
```bash
DEEPGRAM_API_KEY=xxx
GEMINI_API_KEY=xxx
LOG_LEVEL=INFO
CORS_ORIGINS=https://yourdomain.com
```

### Production Settings
```python
uvicorn.run(
    "main:app",
    host="0.0.0.0",
    port=8000,
    workers=4,
    log_level="info",
    access_log=True
)
```

## Troubleshooting

### High Latency
- Check network to Deepgram/Gemini APIs
- Verify audio chunk size (4096 samples = 256ms at 16kHz)
- Monitor CPU usage (transcoding can be intensive)

### Audio Quality Issues
- Ensure 16kHz input, 24kHz output
- Check for buffer underruns
- Verify base64 encoding/decoding

### Connection Drops
- Check firewall settings
- Verify WebSocket keep-alive
- Monitor Deepgram connection status

### No Audio Output
- Verify TTS model: `aura-asteria-en`
- Check audio format: PCM Int16
- Ensure sample rate: 24kHz

## Performance Optimization

### 1. Reduce Audio Chunk Size
```python
# Smaller chunks = lower latency, higher overhead
processor = audioContext.createScriptProcessorNode(2048, 1, 1)  # 128ms at 16kHz
```

### 2. Parallel Processing
```python
# Process STT and TTS concurrently
await asyncio.gather(
    process_stt(audio),
    generate_tts(text)
)
```

### 3. Connection Pooling
```python
# Reuse HTTP connections
session = aiohttp.ClientSession()
```

### 4. Regional Endpoints
```python
# Use Singapore region for Asia-Pacific
deepgram_client = DeepgramClient(
    api_key,
    config={"region": "ap-southeast-1"}
)
```

## License

MIT

## Support

For issues or questions:
- GitHub Issues: [your-repo]
- Email: support@yourdomain.com
- Docs: https://docs.yourdomain.com
