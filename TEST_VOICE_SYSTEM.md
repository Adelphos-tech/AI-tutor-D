# ğŸ§ª Voice System Test Guide

## âœ… Stack Status

### Backend (Port 8000)
- âœ… **Running**: FastAPI server active
- âœ… **Health**: `/health` endpoint responding
- âœ… **WebSocket**: `/ws` accepting connections
- âœ… **Deepgram STT**: Nova-3 connected
- âœ… **Gemini 2.5 Pro**: Initialized
- âœ… **Deepgram TTS**: Aura-1 ready

### Frontend (Port 3000)
- âœ… **Running**: Next.js server active
- âœ… **Database**: Connected
- âœ… **Component**: VoiceTeacherRealtime loaded

---

## ğŸ¯ Manual Test Steps

### Step 1: Verify Backend
```bash
curl http://localhost:8000/health
# Should return: {"status":"healthy"}
```

### Step 2: Open Frontend
```
http://localhost:3000/library
```

### Step 3: Upload/Select Document
1. Upload a PDF or select existing material
2. Click on the material to open it

### Step 4: Start Voice Session
1. Click **"Voice Teacher"** tab
2. Click **"Start Realtime Session"**
3. Wait for **green "Ready"** badge

### Step 5: Test Speaking
1. Click **"Start Talking"** button
2. Speak clearly: **"Hello, can you hear me?"**
3. Watch for:
   - ğŸ™ï¸ Red "Listening..." badge
   - ğŸ“ Live transcript appearing
   - ğŸ”µ Blue "AI Speaking..." badge
   - ğŸ”Š Audio response playing

---

## ğŸ“Š What to Monitor

### Browser Console (F12)
```
âœ… Connected to voice backend
ğŸ“Š Status: connected
ğŸ™ï¸ Starting recording...
âœ… Recording started
ğŸ“ Transcript: Hello (interim)
ğŸ“ Transcript: Hello, can you hear me? (final)
ğŸ’¬ Text chunk: Hi there!
ğŸ”Š Playing audio chunk
```

### Backend Logs
```bash
# In voice-backend directory:
tail -f /tmp/server.log

# Or check the running process output
```

Expected logs:
```
ğŸ”Œ Client connected
âœ… Deepgram STT (Nova-3) connected
âœ… Gemini 2.5 Pro initialized
âœ… Deepgram TTS (Aura-1) ready
ğŸš€ All streaming connections initialized
ğŸ™ï¸ Received audio chunk: XXXX bytes
âœ… Sent XXXX bytes to Deepgram STT
ğŸ“ Transcript: Hello, can you hear me?
ğŸ¤ Utterance end detected
ğŸ§  Generating response for: Hello, can you hear me?
ğŸ”Š Converting to speech: Hi there!...
âœ… TTS chunk complete
```

---

## ğŸ› Troubleshooting

### No "Ready" Badge
**Problem**: Connection not establishing

**Check**:
```bash
# Backend running?
curl http://localhost:8000/health

# WebSocket accessible?
wscat -c ws://localhost:8000/ws
```

**Fix**: Restart backend
```bash
cd voice-backend
./start.sh
```

### No Transcript Appearing
**Problem**: STT not receiving audio

**Check Browser Console**:
- Look for "Recording started"
- Check microphone permissions
- Verify audio chunks being sent

**Check Backend Logs**:
- Look for "Received audio chunk"
- Check Deepgram connection status

### No AI Response
**Problem**: LLM or TTS failing

**Check Backend Logs**:
- Look for "Generating response"
- Check for Gemini errors
- Verify API keys in `.env`

### No Audio Playback
**Problem**: TTS or playback failing

**Check**:
- Browser audio permissions
- System volume
- Look for "Playing audio chunk" in console

---

## ğŸ¯ Expected Latency

| Stage | Target | Acceptable |
|-------|--------|------------|
| **Connection** | <500ms | <1s |
| **STT (Speech â†’ Text)** | 150-200ms | <500ms |
| **LLM (First Token)** | 400-500ms | <1s |
| **TTS (Text â†’ Audio)** | 250-300ms | <500ms |
| **Total (End-to-End)** | **<1s** | **<2s** |

---

## âœ… Success Criteria

Your system is working when:

1. âœ… Backend health check passes
2. âœ… WebSocket connects (green badge)
3. âœ… Recording starts (red badge)
4. âœ… Live transcript appears as you speak
5. âœ… AI responds within 1-2 seconds
6. âœ… Audio plays smoothly
7. âœ… No errors in console or logs

---

## ğŸš€ Test Scenarios

### Scenario 1: Simple Greeting
**Say**: "Hello"

**Expected**:
- Transcript: "Hello"
- Response: "Hi there! I'm Alex, your AI tutor..."
- Latency: <1 second

### Scenario 2: Question
**Say**: "What is machine learning?"

**Expected**:
- Transcript: "What is machine learning?"
- Response: Detailed explanation
- Latency: 1-2 seconds

### Scenario 3: Document Question
**Say**: "What is chapter 3 about?"

**Expected**:
- Transcript: "What is chapter 3 about?"
- Backend searches document
- Response: Content from chapter 3
- Latency: 2-3 seconds

### Scenario 4: Barge-in (Interrupt)
1. Start speaking
2. While AI is responding, speak again
3. AI should stop and listen to new input

---

## ğŸ“ Test Results

Date: _______________

| Test | Status | Notes |
|------|--------|-------|
| Backend Health | â¬œ Pass â¬œ Fail | |
| WebSocket Connect | â¬œ Pass â¬œ Fail | |
| STT (Transcription) | â¬œ Pass â¬œ Fail | |
| LLM (Response) | â¬œ Pass â¬œ Fail | |
| TTS (Audio) | â¬œ Pass â¬œ Fail | |
| End-to-End Latency | _____ ms | |
| Barge-in | â¬œ Pass â¬œ Fail | |

---

## ğŸ‰ Next Steps

Once all tests pass:

1. âœ… Test with different questions
2. âœ… Test with uploaded documents
3. âœ… Test barge-in functionality
4. âœ… Measure actual latency
5. âœ… Test in different browsers
6. âœ… Test with different microphones

---

## ğŸ“ Quick Commands

```bash
# Check backend health
curl http://localhost:8000/health

# Restart backend
cd voice-backend && ./start.sh

# Restart frontend
cd intellitutor && npm run dev

# View backend logs
tail -f voice-backend/logs/*.log

# Test WebSocket
wscat -c ws://localhost:8000/ws
```

---

**Your real-time voice AI system is ready for testing!** ğŸš€

Follow the steps above and report any issues you encounter.
